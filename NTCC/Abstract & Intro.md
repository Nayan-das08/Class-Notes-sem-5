>Chapter - [[_NTCC 2nd yr]]

# Abstract
This paper discusses the application of supervised machine learning technique for solving PDE initial value and boundary value problems. we have used Artificial Neural Networks (ANN) for this problem, in which a well trained model maps the values of independent variables of a function to get the required solution for the PDE with appropriate constraints. There have been many significant research in this field and most of them use ANN approach for their models. Although the papers published provide formidable solutions, but the model proposed here is a rather simple and straightforward approach to the problem, which manages to give results with appreciable accuracy. 

# Introduction
Partial Differential Equations is an indispensable part of physics, engineering and economics. They are used extensively for simulation, modelling and predicting outcomes of experiments. Most of natural phenomena observed and studied in physics like heat, vibrations, fluid flow, etc. are governed by PDEs. Thus it is important that there exist some way to find the solution to the equations. While some PDEs are easy to find solution for, some are notoriously difficult. For example Navier-Stokes equations, which describe the flow of viscous fluids does not have any proof of infinitely differentiable solutions existing in all three dimensions for all points belonging to the domain. Thus, this is an open mathematical problem and has been termed as one of the Millennium Prize Problems by the Clay Mathematics Institute. But some solutions which are differentiable in at least two or one dimensions can be found using these new techniques involving machine learning.

But solving these equations manually or even numerically with appreciable accuracy can be challenging as ensuring availability of appropriate and well labelled data is a difficult task. Also, creating a complex model which can give the most appropriate result with least number of samples and under least amount of time taken, while maintaining the architecture of the network minimal is a necessity. 

Thus, this paper proposes a simple approach to solve differential equations, namely the 1D wave equation, with the help of feedforward Neural Network models. The dataset required for training and testing the model will be created using the actual solution of said equation.


# new intro
- [x] PDEs
- [x] PDE applications
- [x] Neural Networks
- [x] Deep Learning
- [x] Latest developments

## PDE
Equations which establishes a relationship between an unknown function and its derivatives with respect to its independent variables is called a Differential equation. When the unknown function is is a multivariate function and the equation contains its partial derivatives, then the differential equation is called Partial Differential Equation (PDE). These are solved for their solutions using various methods. PDEs are classified into several types on the basis of their structure. This include linear, non-linear, homogenous and non-homogenous. Among these, non-linear PDE exhibit very complex behaviour and model several natural phenomena which may be seen to layman eyes as chaos. Different types of PDEs have different methods for determining their solution. For instance, Lagrange method is used for solving linear PDEs and Charpit method is applied on non-linear PDEs. Another method called "separation of variables" is used to solve for those solutions which are sum or products of single-variable functions. This is applied on linear homogeneous PDEs. The form of solution obtained from this method is *insert u = X(x)T(t)* and is used for wave wquation, heat equation, Laplace equation, Helmholtz equations and several other common forms of equations.

## PDE Applications
There is a wide range of applications for PDEs in physics and engineering. These generally include modelling scientific processes to analyse their effect in certain conditions. Since, a variety of physical phenomena are governed by known PDEs, solving for their solution becomes important for studying them. For example, the diffusion process is described by the Heat equation defined by Joseph Fourier, the application of Navier-Stokes and Burgers' equations in fluid dynamics and several other applications in the field of engineering like traffic flow, elasticity, harmonics and so on. Among other fields where PDEs are implemented biology has many modelling problems where the processes are governed by certain PDEs like monitoring biological and chemical oscillations with the help of Kuramoto model, the McKendrick-von Foerster equation for age modelling and cell proliferation. Even the commonly used logistic or sigmoid function has its roots with Verhulst equation describing the biological population growth and was used for modelling COVID-19 cases in the early phases of the pandemic. Interestingly, the world of economics and finance is also familiar with the impact of PDEs as equations like Black-Scholes and Fokker-Planck are used extensively to model the market growth. Since, PDEs are such an indispensable component for so many fields and disciplines, studying them and their solutions is imperative. 

## Neural Networks
The development of Artificial Neural Networks (ANN) is one of the most crucial advancements in the field of Machine Learning and AI. The structure and design is inspired by one of the most powerful computational tool known to humankind - the human brain. The billions of neurons in the brain fire constantly to help in making the decisions while we get around to accomplish our everyday tasks. The architecture of ANN is fashioned in a similar way with three major components - the input layer, the hidden layers and the output layer. The input layer receives the data from the developer. This can either be well labelled data for supervised techniques or unlabelled and unfiltered data for the application of unsupervised methods, it entirely depends on the requirement. The hidden layers are layers of neurons or nodes stacked between input layer and the output layer. These help the model to learn various aspects of the data and ultimately a model is developed which gives outputs with least possible error and highest possible accuracy for given constraints. The output layer is the last layer where the nodes receive the final values which in fact are the predicted results. But the architecture alone cannot enable a model to achieve machine learning capabilities. Thus, it requires other features. To help ANN learn linear and non-linear trends in data, linear and non-linear properties must be introduced to the model. This is done with the help of the weighted sum and the activation function. The weighted sum is the sum of products of values incoming from predecessor layer and the weights associated with each connection between the nodes of the two layers in question. This helps teaching linear relationships to the model. This weighted sum is then put in a function called Activation Function which provides the model with non-linear properties. While traditional activation functions - sigmoid and hyperbolic tangent functions are still relevant, new functions like Rectified Linear Unit (ReLU), Leaky ReLU, Softmax, etc. are also implemented heavily as per requirement. 

Now, although the model might be ready to give outputs on the supplied input data, there would not be any learning taking place. We implement the concept of optimization for making the ANN framework to start learning and start giving better results with each iterations. For that, we define a loss function which best suits our need. The loss function would define how far away the predicted solution is from the actual solution. There are several classes of loss functions available which can be tailored and altered as required. The most commonly used loss functions for regression problems are Mean Squared Loss, Mean Absolute Loss, Sum of Squared Loss, etc., while Binary Cross-Entropy and Sparse Cross-Entropy are used for classification problems. The ultimate loss values for an iteration is then optimized using an optimizing algorithm. The developer may choose from a huge library of algorithms. Most of these are Gradient Descent based algorithms. Thus, a need arises to calculate the gradient of the loss function with respect to the weights and biases corresponding to each connection. Here, the concept of Backpropagation comes to rescue. It is method to calculate the gradient of the loss function for all the weights, starting from the last layer to the first layer of architecture. For calculation of gradients it enlists the help of Automatic Differentiation algorithm. Auto-differentiation is such an integral part of Neural Networks modelling that all popular ANN environments support this feature. For instance, in Tensorflow, the implementation is provided with GradientTape() (explored briefly in Methodology). Thus, with Backpropagation and continuous optimization of loss function for the weight matrices, the ANN model should give excellent predicted results provided appropriate number of iterations take place.

## Deep Learning
Although, a simple Multi-Perceptron ANN model would give very good results, complex problems require a lot of computation and thus they require several hidden layers as one or two layer may not suffice the computational requirement for the problem. Thus, implementation of multiple layers containing nodes capable of processing data in non-linear fashion is called Deep Learning. It is considered one of the most promising implementations of AI and is capable of achieving levels of accuracy other models may fail to reach. With technological advancements in the field of processors with the introduction of high-speed GPUs and easy-to-use ANN environments, Deep Learning is able to achieve wonders. Some recent developments in the field are Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) and Modular Neural Network.

## our model
In this project I have created a simple and minimalistic Neural Network model to solve PDE and demonstrate the model on classical wave equation. The model is trained using evenly spaced vector created on the domain for space and time variables and the training dataset is generated using the actual solution of the equation. We test the model for different number of training samples and for different values of epochs for training and describe the result obtained with the help of plots and tables. 

# new abstract
In this project I have proposed a simple Artificial Neural Network (ANN) framework which is used to solve IVP and BVP problems for PDEs. Since, there are many robust models available for solving such problems, this project is created with the intention of creating a computationally minimalistic framework which can solve any PDEs provided their constraints and the form of solution is known. A labelled master dataset is constructed on the domain, from which, samples were randomly selected to create the training set. The ANN sequential model created created using Tensorflow and Keras employs the cost function, the optimizing algorithm and the architecture were selected posterior to meticulous experimenting. The model is trained for a set number of epochs and for a particular sample size in order to study and demonstrate their impact. The result is displayed in the form of comparative plots to view the learning capabilities of the model. The time taken by a model for learning and the mean error in each model is also tabulated for analysis. The literature review for the project is presented with discussions on improvements and other techniques by researchers in this field. 


## methodology in short
* dataset
	* [x] function for obtaining solution u(x,t) for some (x,t)
	* [x] randomly choosing samples
* model
	* [x] sequential
	* [x] RMSprop
	* [x] MSE
	* [x] architecture
* training
	* [x] epochs
	* [x] sample size
* testing
	* [ ] plots
	* [ ] errors
	* [ ] time


In this project, we have developed convolutional neural networks (CNN) for a facial expression recognition task. The goal is to classify each facial image into one of the seven facial emotion categories considered in this study. We trained CNN models with different depth using gray-scale images from the Kaggle website. We developed our models in Torch and exploited Graphics Processing Unit (GPU) computation in order to expedite the training process. In addition to the networks performing based on raw pixel data, we employed a hybrid feature strategy by which we trained a novel CNN model with the combination of raw pixel data and Histogram of Oriented Gradients (HOG) features. To reduce the overfitting of the models, we utilized different techniques including dropout and batch normalization in addition to L2 regularization. We applied cross validation to determine the optimal hyper-parameters and evaluated the performance of the developed models by looking at their training histories. We also present the visualization of different layers of a network to show what features of a face can be learned by CNN models.